<!DOCTYPE html>
<!-- adapted from http://bayesiandeeplearning.org/ -->
<html class="mel_workshop"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>ICDM WORKSHOP</title>
		<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
		<meta http-equiv="Pragma" content="no-cache">
		<meta http-equiv="Expires" content="0">
		<meta name="description" content="ICDM WORKSHOP">
		<meta name="keywords" content="ICDM WORKSHOP">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="main.css">
		<meta property="og:title" content="ICDM WORKSHOP">
		<meta property="og:type" content="website">
		<meta property="og:url" content="http://creativeai-ws.org">
		<meta property="og:description" content="ICDM WORKSHOP">
	</head>
	<body data-gr-c-s-loaded="true" class="">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<h1>International Workshop on Data Mining for Education (DME): Techniques, Challenges, and Applications</h1>
						<h2><b>ICDM WORKSHOP</b></h2>
						<h2>The workshop will be co-hosted with the IEEE International Conference on Data Mining (ICDM) 2023, December 1--4, in Shanghai, China.</h2>
						<h2>DME will be a **half-day** workshop on December 4th, 2023.</h2>
						<!--<h2><a href="https://iclr.cc/virtual/2021/workshop/2134" target="_blank"><font color="red">Link to ICLR Workshop Virtual Site (Join Zoom)</font></a></h2>-->
					</header>

				<!-- Nav -->
					<nav id="nav" class="">
						<ul>
							<li><a href="#abstract" class="active">SCOPE</a></li>
							<li><a href="#speakers" class="">TOPICS</a></li>
							<li><a href="#accepted" class="">COMMITTEE</a></li>
<!--							<li><a href="#schedule" class="">Schedule</a></li>-->
							<li><a href="#cfp" class="">SUBMISSION</a></li>
							<li><a href="#organizers" class="">PUBLICATION</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="abstract" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
												<h2>SCOPE</h2>
											</center>
										</header>
										<h3 style="text-align:justify">Data mining and big data analytics have significant potential for improving learning outcomes and supporting decision making in educational systems. By analyzing large amounts of data, researchers and educators can gain insights into student learning conditions and behavior, as well as identify patterns and trends that can inform instructional strategies and improve educational outcomes. Data can be leveraged by researchers to validate education and research findings at a larger scale, leading to a better understanding of student learning conditions and improved teaching support. Educators can monitor student progress and enhance the teaching process, while students can benefit from more effective course selection and educational management. Additionally, with the aid of large amounts of data, predictions regarding student dropout rates, motivations, and diversity can be significantly enhanced. It also becomes possible to gain a more comprehensive understanding of particular student groups, ultimately resulting in improved adaptivity and personalization for individual students. However, it is important to recognize that data mining poses risks to user privacy and security. As educational institutions collect and store large amounts of student data, it is important to ensure that this data is secure and protected from unauthorized access or misuse.</h3>
<!--										<h3>-->
<!--										For the past few years, we have witnessed eye-opening generation results from AI foundation models such as GPT-3, and DALL-E2. These models have set up great infrastructures for new types of creative generation across various modalities such as language (e.g. story generation), images (e.g. text-to-image generation, fashion design), and audio (e.g. lyrics-to-music generation). Researchers in these fields encounter many similar challenges such as how to use AI to help professional creators, how to evaluate creativity for an AI system, how to boost the creativity of AI, how to avoid negative social impact, and so on. There have been various workshops that focus on some aspects of AI generation. This workshop aims to bridge researchers and practitioners from NLP, computer vision, music, ML, and other computational fields to create the 1st workshop on “Creative AI across Modalities”.-->
<!--										</h3>-->


									</div>
									<!-- <span class="image"></span> -->
								</div>
							</section>

<section id="speakers" class="main">
	<div>
	<div class="content">
		<header class="major">
			<center>
				<h2>ACCEPTED PAPERS</h2>
			</center>
		</header>
		<p>Intelligent Practical Teaching Platform based on Data Mining</p>
		<u>Qi Liu, Xiao Chen, Kun Niu, Wanru Zhang, Yiman Gao, and Jin Wei</u>
		<br>
		<p>A Deep Memory-Aware Attentive Model for Knowledge Tracing</p>
		<u>Juntai Shi, Wei Su, Lei Liu, Shenglin Xu, Tianyuan Huang, Jiamin Liu, Wenli Yue, and Shihua Li</u>
		<br>
		<p>Optimization and Improvement of Fake News Detection using Voting Technique for Societal Benefit</p>
		<u>Sribala Chinta, Karen Farnandes, Ningxi Cheng, Zhipeng Yin, Zichong Wang, CHONG Siang Yew, Puqing Jiang, and Wenbin Zhang</u>
		<br>
		<p>Exploring Approaches for Teaching Cybersecurity and AI for K-12</p>
		<u>Yu Cai and Drew Youngstrom</u>

		</div>
		</div>
	</center>
</section>


							<section id="speakers" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
												<h2>TOPICS</h2>
											</center>
										</header>
										<p style="text-align:justify">
              The purpose of this workshop is to unite researchers from various fields such as data mining, big data,
machine learning, security, privacy, and cognitive science. Our objective is to foster a discussion and
exchange of ideas that focuses on innovative and pragmatic research and educational approaches,
methods, and obstacles related to data mining for education. We welcome submissions of papers
covering a wide range of topics of interest, including but not limited to:
            </p>
            <ul>
              <li>Data mining and big data analytics for personalized learning and adaptive teaching</li>
              <li>Predictive analytics for identifying at-risk students and enhancing student success</li>
              <li>Machine learning techniques for educational data analysis</li>
              <li>Comparative analysis of different data mining algorithms in educational settings</li>
              <li>Data visualization for educational data mining</li>
              <li>Educational data mining for curriculum design and development</li>
              <li>Data mining for measuring and improving student engagement</li>
              <li>Educational data mining for teacher professional development and support</li>
              <li>Security and privacy issues related to educational data mining</li>
              <li>Ethical and legal considerations in educational data mining</li>
              <li>Educational data mining for decision-making and policy development in education</li>
              <li>Impact of educational data mining on equity and inclusivity in education.</li>
              <li>Novel approaches and challenges in educational data mining</li>
            </ul>
										<center>
<!--										<div class="row uniform" align="center">-->
<!--					 						<div class="3u 12u$(small)">-->
<!--												<a href="https://sites.google.com/site/snigdhac/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/snigdha.jpeg" alt="">-->
<!--													</span>-->
<!--													<h2>Snigdha Chaturvedi<br> (UNC) </h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="3u 12u$(small)">-->
<!--											<a href="https://chrisdonahue.com/" target="_blank" class="image">-->
<!--												<span class="image fit">-->
<!--													<img src="./CAIAM_files/chris.png" alt="">-->
<!--												</span>-->
<!--												<h2>Chris Donahue<br> (Google Magenta) </h2>-->
<!--											</a>-->
<!--											</div>-->
<!--					 						<div class="3u 12u$(small)">-->
<!--												<a href="https://andrewowens.com/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/andrew.jpg" alt="">-->
<!--													</span>-->
<!--													<h2>Andrew Owens<br> (UMich) </h2>-->
<!--												</a>-->
<!--											</div>-->
<!--										</div>-->
<!--										<div class="row uniform" align="center">-->
<!--					 						<div class="3u 12u$(small)">-->
<!--												<a href="https://kittur.org/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/niki.png" alt="">-->
<!--													</span>-->
<!--													<h2>Niki Kittur<br> (CMU) </h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="3u 12u$(small)">-->
<!--												<a href="http://eilab.gatech.edu/mark-riedl.html" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/mark_2.png" alt="">-->
<!--													</span>-->
<!--													<h2>Mark Riedl<br> (Georgia Tech) </h2>-->
<!--												</a>-->
<!--											</div>-->
<!--					 						<div class="3u 12u$(small)">-->
<!--												<a href="https://cs.stanford.edu/~diyiy/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/diyi.jpeg" alt="">-->
<!--													</span>-->
<!--													<h2>Diyi Yang<br> (Stanford) </h2>-->
<!--												</a>-->
<!--											</div>-->
<!--					 						<div class="3u 12u$(small)">-->
<!--												<a href="https://www.dgp.toronto.edu/~hertzman/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/aarong.png" alt="">-->
<!--													</span>-->
<!--													<h2>Aaron Hertzman<br> (Adobe Research) </h2>-->
<!--												</a>-->
<!--											</div>-->
									</div>
								</div></center>
							</section>




							<section id='accepted' class='main'>
								<header class="major">
											<center>
											<h2>COMMITTEE</h2>
											</center>
										</header>
											<h2><b>Workshop Chairs:</b></h2>
											<ul>
												<li>Yu Cai, Michigan Technological University, Michigan, USA</li>
												<li>Wenbin Zhang, Michigan Technological University, Michigan, USA</li>
											</ul>

             <h2><b>Technical Committee Members:</b></h2>
								<ul>
									<li>Heitor Murilo Gomes, Victoria University of Wellington</li>
            <li>Israat Haque, Dalhousie University</li>
            <li>Jun Liu, Carnegie Mellon University</li>
            <li>Liang Luo, University of Electronic Science and Technology</li>
            <li>Yang Liu, Meta Platforms</li>
            <li>Yu Pang, Chongqing University of Posts and Telecommunications</li>
            <li>Zhipeng Huang, Case Western Reserve University</li>
            <li>Shiwen Ni, Chinese Academy of Sciences</li>
								</ul>

<!--								<ul>-->
<!--									<li><a href="https://openreview.net/forum?id=UQY0bqcl_mX">Photong: Generating 16-Bar Melodies from Images</a> By: Yanjia Zhang, Haohan Wang</li>-->
<!--<li><a href="https://openreview.net/forum?id=haiht1U7pGL">Large Language Models Learn to Drum</a> By: Li Zhang, Chris Callison-Burch</li>-->
<!--<li><a href="https://openreview.net/forum?id=Nx9ajnqG9Rw">Blind Judgement: Agent-Based Supreme Court Modelling with GPT</a> By: Sil Hamilton</li>-->
<!--<li><a href="https://openreview.net/forum?id=UqvWNBQKf5">A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the Input is Under-Specified?</a> By: Kathleen C. Fraser, Isar Nejadgholi, Svetlana Kiritchenko</li>-->
<!--<li><a href="https://openreview.net/forum?id=M24Cs12Gq_A">Spiking ConvLSTM for Semantic Music Generation</a> By: Anna Shvets</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/8_help_me_write_a_poem_instructi.pdf">Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing</a> By: Tuhin Chakrabarty, Vishakh Padmakumar, He He</li>-->
<!--<li><a href="https://openreview.net/forum?id=QmWXskBhesn">Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Tune Generation Task</a> By: Shangda Wu, Maosong Sun</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/10_towards_grounded_dialogue_gene.pdf">Towards Grounded Dialogue Generation in Video Game Environments</a> By: Nader Akoury, Ronan Salz, Mohit Iyyer</li>-->
<!--<li><a href="https://openreview.net/forum?id=8HwKaJ1wvl">Improving the Creativity of Generative Language Models</a> By: Douglas Summers-Stay, Clare R. Voss, Stephanie M. Lukin</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/13_is_ai_art_another_industrial_r.pdf">Is AI Art Another Industrial Revolution in the Making?</a> By: Alexis Newton, Kaustubh Dhole</li>-->
<!--<li><a href="https://openreview.net/forum?id=nmtmjfJQLS">Music Playlist Title Generation Using Artist Information</a> By: Haven Kim, Seungheon Doh, Junwon Lee, Juhan Nam</li>-->
<!--<li><a href="https://openreview.net/forum?id=AtGBXiOAGY">3DStyleMerge: Part-Compatible 3D Style Transfer</a> By: Abhinav Upadhyay, Alpana Dubey, Suma Mani Kuriakose</li>-->
<!--<li><a href="https://openreview.net/forum?id=tqCf3xklDG">Color Me Intrigued: Quantifying Usage of Colors in Fiction</a> By: Siyan Li</li>-->
<!--<li><a href="https://openreview.net/forum?id=wm0WZPnhTC">Trash to Treasure: Using text-to-image models to inform the design of physical artefacts</a> By: Amy Smith, Hope Schroeder, Ziv Epstein, Mike Cook, Simon Colton, Andrew Lippman</li>-->
<!--<li><a href="">Unsupervised Melody-Guided Lyrics Generation</a></li>-->
<!--<li><a href="https://openreview.net/forum?id=vuqI2p_ZQT">Exploiting Multiple Guidance From 3DMM For Face Reenactment</a> By: Huayu Zhang, Yurui Ren, Yuanqi Chen, Ge Li, Thomas H. Li</li>-->
<!--<li><a href="https://openreview.net/forum?id=cLBEKlu5WZK">Neural Story Planning</a> By: Anbang Ye, Christopher Zhang Cui, Taiwei Shi, Mark Riedl</li>-->
<!--<li><a href="https://openreview.net/forum?id=9lmAR2NjTt">Leveraging Human Preferences to Master Poetry</a> By: Rafael Pardinas, Gabriel Huang, David Vazquez, Alexandre Piché</li>-->
<!--<li><a href="https://openreview.net/forum?id=_8Ity3P03Z1">SEE&TELL: Controllable Narrative Generation from Images</a> By: Stephanie M. Lukin, Sungmin Eum</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/26_learning_the_visualness_of_tex.pdf">Learning the Visualness of Text Using Large Vision-Language Models</a> By: Gaurav Verma, Ryan A. Rossi, Christopher Tensmeyer, Jiuxiang Gu, Ani Nenkova</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/27_sketchbetween_video_to_video_s.pdf">SketchBetween: Video-to-Video Synthesis for Sprite Animation via Sketches</a> By: Dagmar Lukka Loftsdóttir, Matthew Guzdial</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/28_threshold_designer_adaptation_.pdf">Threshold Designer Adaptation: Improved Adaptation for Designers in Co-creative Systems</a> By: Emily Halina, Matthew Guzdial</li>-->
<!--<li><a href="https://openreview.net/forum?id=UMxeP-FuwyC">Simple Unsupervised Image Captioning via CLIP’s Multimodal Embeddings</a> By: Derek Tam, Colin Raffel, Mohit Bansal</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/31_culturally_aware_stable_diffus.pdf">Culturally-Aware Stable Diffusion: Supporting Cultural Representation in Text-to-Image Synthesis</a> By: Zhixuan Liu, Peter Schaldenbrand, Youeun Shin, Beverley-Claire Okogwu, Youngsik Yun, Jihie Kim, Jean Oh</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/33_robot_synesthesia_a_sound_and_.pdf">Robot Synesthesia: A Sound and Semantics Guided AI Painter</a> By: Vihaan Misra, Peter Schaldenbrand, Jean Oh</li>-->
<!--<li><a href="https://openreview.net/forum?id=phc0KisUnS">Deep Generative Multimedia Children's Literature</a> By: Matthew Lyle Olson</li>-->
<!--<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/36_diffusion_models_as_visual_rea.pdf">Diffusion Models as Visual Reasoners</a> By: Jason Lin, Maya Srikanth</li>-->
<!--<li><a href="https://openreview.net/forum?id=3hk5PFxQSG">In BLOOM: Evaluating Creativity and Affinity in Artificial Lyrics and Art</a> By: Evan Crothers, Herna L. Viktor, Nathalie Japkowicz</li>-->
<!--<li><a href="https://openreview.net/forum?id=FPDRONopLH">Conveying the Predicted Future to Users: A Case Study of Story Plot Prediction</a> By: Chieh-Yang Huang, Saniya Naphade, Kavya Laalasa Karanam, Ting-Hao Huang</li>-->
<!--<li><a href="https://openreview.net/forum?id=c7W3iTr693">A Tool for Composing Music via Graphic Scores in the style of Gyorgy Ligeti's Artikulation using Self-supervised Representation Learning</a> By: Berker Banar, Simon Colton</li>-->
<!--<li><a href="https://openreview.net/forum?id=w7n0i0Y4xy">One Artist’s Personal Reflections on Methods and Ethics of Creating Mixed Media Artificial Intelligence Art</a> By: Jane Adams</li>-->
<!--								</ul>-->

							</section>





<!--							<section id="schedule" class="main">-->
<!--								<div>-->
<!--									<div class="content">-->
<!--										<header class="major">-->
<!--											<center>-->
<!--											<h2>Schedule</h2>-->
<!--                                                                                        For the in-person participants, please go to Room 146B for joining all the talks and the poster session.-->
<!--											</center>-->
<!--										</header>-->

<!--										<div class="table-wrapper">-->
<!--											<table class="alt">-->
<!--												<tbody>-->
<!--                                                    <col width="15%">-->
<!--                                                    <col width="15%">-->
<!--                                                    <col width="15%">-->
<!--													<tr>-->
<!--														<td>08:50 am - 09:00 am (EST)</td>-->
<!--														<td>Introduction and Opening Remarks</td>-->
<!--														<td></td>-->
<!--														<td></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>09:00 am - 09:45 am (EST)</td>-->
<!--														<td>Invited Talk</td>-->
<!--														<td>Andrew Owens<br><font size="2">(UMich)</font></td>-->
<!--														<td>Title: Cross-modal Synthesis with Sight, Sound, and Touch</td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>09:45 am - 10:30 am (EST)</td>-->
<!--														<td>Invited Talk</td>-->
<!--														<td>Mark Riedl<br><font size="2">(Georgia Tech)</font></td>-->
<!--														<td>Title: Computers, Creativity, and Lovelace <br> <font size="2">Abstract: In this talk we examine the what attributes we should expect in human-level creative systems, and the mechanisms by which we might achieve them. I provide examples from the domain of automated story generation. I conclude the talk with some informal analysis of recent progress toward AI systems that express creativity. </font></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>10:30 am - 10:40 am (EST)</td>-->
<!--														<td>Break</td>-->
<!--														<td></td>-->
<!--														<td></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>10:40 am - 11:25 am (EST) </td>-->
<!--														<td>Invited Talk</td>-->
<!--														<td>Chris Donahue<br><font size="2">(Google)</font></td>-->
<!--														<td>Title: Frontiers in Controllable Music Generation <br> <font size="2">Abstract: For music generation and creative generation more broadly, control is key to unlocking human expression. In this talk, I will discuss the recent improvements in and remaining obstacles to building controllable music generation systems that unlock exciting new expressive capabilities for musicians and non-musicians alike. Additionally, I will discuss control considerations that are more specific to music and argue that text is useful but not sufficient for expressive musical control. As a case study, I will discuss SingSong, a recent system from the MusicLM project at Google which learns to translate vocal performances into instrumental accompaniments, thereby allowing anyone to create rich music featuring their own voice.</font></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>11:25 am - 12:10 pm (EST) </td>-->
<!--														<td>Invited Talk</td>-->
<!--														<td>Niki Kittur<br><font size="2">(CMU)</font></td>-->
<!--														<td>Title: Scaling Analogical Innovation</td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>12:10 pm - 01:30 pm (EST) </td>-->
<!--														<td>Lunch</td>-->
<!--														<td></td>-->
<!--														<td></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>01:30 pm - 02:50 pm (EST) </td>-->
<!--														<td><b>Poster Session (virtual + in person)</b></td>-->
<!--														<td></td>-->
<!--														<td></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>02:50 pm - 3:35 pm (EST)</td>-->
<!--														<td>Invited Talk</td>-->
<!--														<td>Aaron Hertzman<br><font size="2">(Adobe)</font></td>-->
<!--														<td>Title: Can Computers Create Art? <br> <font size="2">Abstract: Can AI algorithms make art, and be considered artists? Within the past decade, the growth of new neural network algorithms has enabled exciting new artforms with considerable public interest, including DeepDream, GANs, VAEs, and diffusion models like DALL-E and Imagen. These tools raise recurring questions about their status as creators and their effect on the arts. In this talk, I will discuss how these developments parallel the development of previous artistic technologies, like oil paint, photography, and traditional computer graphics. I argue that art is a social phenomenon, and discuss possible—but very unlikely—scenarios for when these algorithms could someday be considered artists.</font></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>03:35 pm - 04:20 pm (EST)</td>-->
<!--														<td>Invited Talk</td>-->
<!--														<td>Snigdha Chaturvedi<br><font size="2">(UNC)</font></td>-->
<!--														<td>Title: Modeling People in Automatic Story Generation <br> <font size="2">Abstract: Automatic story generation is the task of designing NLP systems that, given a prompt, can produce the text of a story. Most methods for this problem focus on modeling events and their coherence. However, an alternate perspective to story generation can be from the viewpoint of people described in the story. In this talk, I focus on one aspect of modeling people in story generation &#45;&#45; modeling their social relationships. I describe our story generation approach to incorporate a desired social network demonstrating relationships between various people to be mentioned in the story. We propose a model that uses latent variables to incorporate social relationships. Apart from generating coherent stories that reflect the desired social network, the latent variable-based design results in an explainable generation process. </font></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>04:20 pm - 04:30 pm (EST)</td>-->
<!--														<td>Break</td>-->
<!--														<td></td>-->
<!--														<td></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>04:30 pm - 05:15 pm (EST)</td>-->
<!--														<td>Invited Talk</td>-->
<!--														<td>Diyi Yang<br><font size="2">(Stanford)</font></td>-->
<!--														<td>Title: Improving Everyday Interaction through Human-Centered Text Generation  <br> <font size="2">Abstract: As natural language generation has gained popularity and produced extensive industrial applications, there has been an increasing focus on enabling the use of natural language in human-like interactions. How can we improve such everyday interactions and build language generation systems that are more aware of human factors?  In this talk,  we take a closer look at human-centric language generation and present two recent works that promote positive language use and summarize daily conversations.  Specifically, the first part examines positive reframing by neutralizing a negative point of view and generating a more positive perspective without contradicting the original meaning.  The second part demonstrates how more structures of conversations can be utilized to generate better summaries for everyday conversation.</font></td>-->
<!--													</tr>-->
<!--													<tr>-->
<!--														<td>05:15 pm - 05:25 pm (EST)</td>-->
<!--														<td>Closing Remarks</td>-->
<!--														<td></td>-->
<!--														<td></td>-->
<!--													</tr>-->
<!--												</tbody>-->
<!--											</table>-->
<!--										</div>-->

											<!--<h2>Accepted Papers</h2>
										</center>
	<table>
												<tbody>
<col width="40%">
<col width="60%">
<tr><td><b>
Title
</b></td><td><b>
Authors
</b></td><td><b>
Paper Session
</b></td></tr>
<tr><td><b><a href="./Papers/A1.pdf">ABC Problem: An Investigation of Offline RL for Vision-Based Dynamic Manipulation</a></b></td><td>Kamyar Ghassemipour, Igor Mordatch, Shixiang Shane Gu</td><td><b>A1</b></td></tr>
<tr><td><b><a href="./Papers/A2.pdf">Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal</a></b></td><td>Casey Kennington</td><td><b>A2</b></td></tr>
<tr><td><b><a href="./Papers/A3.pdf">Ask & Explore: Grounded Question Answering for Curiosity-Driven Exploration</a></b></td><td>Jivat Neet Kaur, Yiding Jiang, Paul Pu Liang</td><td><b>A3</b></td></tr>
<tr><td><b><a href="./Papers/A4.pdf">Towards Teaching Machines with Language: Interactive Learning From Only Language Descriptions of Activities</a></b></td><td>Khanh Nguyen, Dipendra Misra, Robert Schapire, Miroslav Dudik, Patrick Shafto</td><td><b>A4</b></td></tr>
<tr><td><b><a href="./Papers/A5.pdf">YouRefIt: Embodied Reference Understanding with Language and Gesture</b></td><td>Yixin Chen, Qing Li, Deqian Kong, Yik Lun Kei, Tao Gao, Yixin Zhu, Song-Chun Zhu, Siyuan Huang</td><td></a><b>A5</b></td></tr>
<tr><td><b><a href="./Papers/B1.pdf">Learning to Set Waypoints for Audio-Visual Navigation</b></td><td>Changan Chen, Sagnik Majumder, Ziad Al-Halah, Ruohan Gao, Santhosh K. Ramakrishnan, Kristen Grauman</td><td></a><b>B1</b></td></tr>
<tr><td><b><a href="./Papers/B2.pdf">Semantic Audio-Visual Navigation</b></td><td>Changan Chen, Ziad Al-Halah, Kristen Grauman</a></td><td><b>B2</b></td></tr>
<tr><td><b>Attentive Feature Reuse for Multi Task Meta learning</b></td><td>Kiran Lekkala, Laurent Itti</a></td><td><b>B3</b></td></tr>
<tr><td><b><a href="./Papers/B4.pdf">SeLaVi: self-labelling videos without any annotations from scratch</b></td><td>Yuki Asano, Mandela Patric, Christian Rupprecht, Andrea Vedaldi</td><td><b>B4</b></td></tr>

												</tbody>
											</table>
									</div>
								</div>
							</section>-->
<section id="cfp" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
											<h2>SUBMISSION</h2>
											</center>
										</header>
										<h3> Paper submissions should be limited to a maximum of 8 pages (excluding references), and follow the
IEEE ICDM format. More detailed information is available in the IEEE ICDM 2023 Submission Guidelines
(<a href="https://www.cloud-conf.net/icdm2023/call-for-papers.html" style="color: #4495C6;">https://www.cloud-conf.net/icdm2023/call-for-papers.html</a>). Submitted manuscripts must not have
been accepted for publication elsewhere or be under review for another workshop, conferences or
journals.
<!--										<li> Long paper: Submission of original work up to seven pages for contents and one page for references.</li>-->
<!--										<li> Short paper: Submission of work in progress with preliminary results, and position papers, up to four pages for contents and one page for references.</li>-->
<!--										Topics including but not limited to:-->
<!--										<ul>-->
<!--												<li>Creative language generation: stories, poetry, figurative languages. </li>-->
<!--												<li>Generative model and algorithms for image/audio, and multi-modal/video generation.</li>-->
<!--												<li>Theory and analysis for creativity (e.g., humor understanding)</li>-->
<!--												<li>Detecting and quantifying creativity</li>-->
<!--												<li> Using AI to improve human creativity (e.g., HCI+ML studies to accelerate scientific novelty)</li>-->
<!--												<li>Data and resources for creative generation</li>-->
<!--												<li>Applications of creative AI generation, such as automatic video dubbing</li>-->
<!--												<li>Novel evaluation for creative AI generated outputs</li>-->
<!--												<li>Social, cultural, and ethical considerations of creative AI generations, such as racial/gender bias, trustworthiness</li>-->
<!--										</ul>-->
<!--											A submission should take the form of a AAAI long or short paper in PDF format using the <a href="https://www.aaai.org/Publications/Templates/AuthorKit23.zip" target="_blank">AAAI style</a>. We will accept submissions of (1) papers that have not been previously published or accepted for publication in substantially similar form; (2) papers that have been published or accepted for publication in recent venues including journal, conference, workshop, and arXiv; and (3) research proposals for future work with a focus on well-defined concepts and ideas. All submissions will be reviewed with double blind policy.-->
										</h3>
<!--										<br>-->

<!--										<h2>Open Review submissions website: <a href="https://openreview.net/group?id=AAAI.org/2023/Workshop/creativeAI">https://openreview.net/group?id=AAAI.org/2023/Workshop/creativeAI</a><h2>-->

<!--										<br>-->
<!--										<h2>Key Dates:-->
<!--											<h3>-->
<!--											<ul>-->
<!--												<li>Submission deadline: Nov. 18, 2022 (11:59 p.m. Anywhere on Earth)</li>-->
<!--												<li>Notification to authors: Dec. 20, 2022 (11:59 p.m. Anywhere on Earth)</li>-->
<!--												<li>Workshop date: Feb. 13, 2023</li>-->
<!--											</ul>-->
<!--											</h3>-->
<!--										</h2>-->


										<!--<h2>Program Committee:
											<h3>
												Unnat Jain (UIUC), Michelle Lee (Stanford), Paul Pu Liang (CMU), Senthil Purushwalkam (CMU), Santhosh Kumar Ramakrishnan (UT Austin), Mohit Shridhar (UW), Tianmin Shu (MIT), Shaoxiong Wang (MIT)
											</h3>
										</h2>-->


									</div>
								</div>
							</section>

							<section id="organizers" class="main special">
								<div>
									<div class="content">
										<header class="major">
											<h2>PUBLICATION</h2>
										</header>
										<h3 style="text-align:justify">All accepted workshop papers will be included in the Proceedings of ICDM Workshop (ICDMW) 2023, published by the IEEE Computer Society Press and each assigned a Digital Object Identifier (DOI). </h3>
										<br>
										<h3 style="text-align:justify">All accepted papers must have at least one “FULL” registration. A full registration is either a “member” or “non-member” registration. Student registrations are not considered full registrations. All authors are required to register by XXX [!date].</h3>
										<br>
										<h3 style="text-align:justify;">For registration queries please contact: <a href="mailto:registration@computer.org" style="color:#4495C6;">registration@computer.org</a></h3>


<!--										<div class="row uniform" align="center">-->
<!--											<div class="1u 0u$(small)">-->
<!--												<a href="#" class="image"></a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://sites.google.com/view/drjinghuang" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/jing.jpg" alt="">-->
<!--													</span>-->
<!--													<h3>Jing Huang <br> (Amazon)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="http://prithvirajva.com/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/raj_1.png" alt="">-->
<!--													</span>-->
<!--													<h3>Prithviraj (Raj) Ammanabrolu<br> (AI2)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://vnpeng.net/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/violet.png" alt="">-->
<!--													</span>-->
<!--													<h3>Violet Peng<br> (UCLA)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://www.cs.unc.edu/~mbansal/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/mohit.png" alt="">-->
<!--													</span>-->
<!--													<h3>Mohit Bansal<br> (UNC)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://jiajunwu.com/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/jiajun.jpeg" alt="">-->
<!--													</span>-->
<!--													<h3>Jiajun Wu<br> (Stanford)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--										</div>-->
<!--										<div class="row uniform" align="center">-->
<!--											<div class="1u 0u$(small)">-->
<!--												<a href="#" class="image"></a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://www.linkedin.com/in/arindam-mandal" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/arindam.jpeg" alt="">-->
<!--													</span>-->
<!--													<h3>Arindam Mandal<br> (Amazon)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://ken77921.github.io/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/hawshiuan.webp" alt="">-->
<!--													</span>-->
<!--													<h3>Haw-Shiuan Chang<br> (Amazon)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://ai.stanford.edu/~rhgao/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/ruohan.jpg" alt="">-->
<!--													</span>-->
<!--													<h3>Ruohan Gao <br> (Stanford)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://users.soe.ucsc.edu/~hannahbrahman/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/faeze.png" alt="">-->
<!--													</span>-->
<!--													<h3>Faeze Brahman <br> (AI2)</h2>-->
<!--												</a>-->
<!--											</div>-->
<!--											<div class="2u 12u$(small)">-->
<!--												<a href="https://cseweb.ucsd.edu/~jmcauley/" target="_blank" class="image">-->
<!--													<span class="image fit">-->
<!--														<img src="./CAIAM_files/julian.jpeg" alt="">-->
<!--													</span>-->
<!--													<h3>Julian McAuley <br> (UCSD)</h2>-->
<!--												</a>-->
<!--											</div>-->

										</div>
									</div>
								</div>
							</section>


					</div>

<!--				 Footer -->
					<footer id="footer">
						<p class="copyright">Last Update: 04/2023</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script async="" src="./MEL_files/analytics.js">
				
			</script><script src="./js/jquery.min.js"></script>
			<script src="./js/jquery.scrollex.min.js"></script>
			<script src="./js/jquery.scrolly.min.js"></script>
			<script src="./js/skel.min.js"></script>
			<script src="./js/util.js"></script>
<!--			[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="./js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JC1QVS8GW2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JC1QVS8GW2');
</script>

</body>
</html>
